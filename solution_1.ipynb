{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11f9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e2009df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 36\n",
      "X: (10000, 273), y1: (10000,), X_eval: (10000, 273)\n",
      "\n",
      "Train/Val Split:\n",
      "X_train: (8000, 273), y_train: (8000,)\n",
      "X_val: (2000, 273), y_val: (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure problem number here\n",
    "PROBLEM_NUM = 36\n",
    "\n",
    "X_path = f\"./data_31_40/problem_{PROBLEM_NUM}/dataset_{PROBLEM_NUM}.csv\"\n",
    "y_path = f\"./data_31_40/problem_{PROBLEM_NUM}/target_{PROBLEM_NUM}.csv\"\n",
    "Xeval_path = f\"./data_31_40/problem_{PROBLEM_NUM}/EVAL_{PROBLEM_NUM}.csv\"\n",
    "\n",
    "X = pd.read_csv(X_path)\n",
    "y = pd.read_csv(y_path)\n",
    "X_eval = pd.read_csv(Xeval_path)\n",
    "\n",
    "y1 = y[\"target01\"]\n",
    "\n",
    "print(f\"Problem {PROBLEM_NUM}\")\n",
    "print(f\"X: {X.shape}, y1: {y1.shape}, X_eval: {X_eval.shape}\")\n",
    "assert list(X.columns) == list(X_eval.columns), \"Train/EVAL column mismatch!\"\n",
    "\n",
    "# Create train/validation split to detect overfitting\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y1, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Val Split:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c7cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X.columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dcdf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda yt, yp: -rmse(yt, yp)  # GridSearchCV maximizes score\n",
    ")\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3681b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # \"LinearRegression\": LinearRegression(),\n",
    "    # \"Ridge\": Ridge(random_state=42),\n",
    "    # \"Lasso\": Lasso(random_state=42),\n",
    "    # \"ElasticNet\": ElasticNet(random_state=42),\n",
    "    # \"SVR\": SVR(),\n",
    "    # \"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    # \"HistGBR\": HistGradientBoostingRegressor(random_state=42),\n",
    "    # \"XGBoost\": XGBRegressor(random_state=42, n_jobs=-1, objective=\"reg:squarederror\"),\n",
    "    \"LightGBM\": LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Optimized hyperparameter grids - reduced search space for faster runtime\n",
    "param_grids = {\n",
    "    \"LinearRegression\": {},\n",
    "\n",
    "    \"Ridge\": {\n",
    "        \"model__alpha\": [0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "\n",
    "    \"Lasso\": {\n",
    "        \"model__alpha\": [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    },\n",
    "\n",
    "    \"ElasticNet\": {\n",
    "        \"model__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "        \"model__l1_ratio\": [0.2, 0.5, 0.8]\n",
    "    },\n",
    "\n",
    "    # \"SVR\": {\n",
    "    #     \"model__C\": [0.1, 1, 10],\n",
    "    #     \"model__epsilon\": [0.01, 0.1, 0.2],\n",
    "    #     \"model__gamma\": [\"scale\"]\n",
    "    # },\n",
    "\n",
    "    \"RandomForest\": {\n",
    "        \"model__n_estimators\": [300],\n",
    "        \"model__max_depth\": [None, 20],\n",
    "        \"model__min_samples_leaf\": [1, 10],\n",
    "        \"model__max_features\": [\"sqrt\"]\n",
    "    },\n",
    "\n",
    "    \"HistGBR\": {\n",
    "        \"model__max_depth\": [None, 6],\n",
    "        \"model__learning_rate\": [0.05, 0.1],\n",
    "        \"model__max_iter\": [500, 1000],\n",
    "        \"model__min_samples_leaf\": [20]\n",
    "    },\n",
    "\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [500, 1000],\n",
    "        \"model__max_depth\": [4, 6],\n",
    "        \"model__learning_rate\": [0.05, 0.1],\n",
    "        \"model__subsample\": [0.8],\n",
    "        \"model__colsample_bytree\": [0.8],\n",
    "    },\n",
    "\n",
    "    \"LightGBM\": {\n",
    "        \"model__n_estimators\": [800, 1500, 2000],\n",
    "        \"model__learning_rate\": [0.01, 0.1],\n",
    "        \"model__max_depth\": [-1, 10],\n",
    "        # \"model__num_leaves\": [31, 63],\n",
    "        # \"model__subsample\": [0.8],\n",
    "        # \"model__colsample_bytree\": [0.8],\n",
    "        # \"model__min_child_samples\": [20],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7757a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Scan (using training data only)\n",
      "==========================================================================================\n",
      "Model            RMSE                 R²                  \n",
      "==========================================================================================\n",
      "LightGBM         0.1109 ± 0.0057      0.7633 ± 0.0235\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scan_results = []\n",
    "\n",
    "print(\"Initial Model Scan (using training data only)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    \n",
    "    # CV on training data only (no data leakage)\n",
    "    rmse_scores = -cross_val_score(pipe, X_train, y_train, cv=cv, scoring=rmse_scorer, n_jobs=-1)\n",
    "    r2_scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='r2', n_jobs=-1)\n",
    "    \n",
    "    scan_results.append((name, rmse_scores.mean(), rmse_scores.std(), \n",
    "                        r2_scores.mean(), r2_scores.std()))\n",
    "\n",
    "scan_results = sorted(scan_results, key=lambda x: x[1])  # lower RMSE better\n",
    "print(f\"{'Model':<16} {'RMSE':<20} {'R²':<20}\")\n",
    "print(\"=\"*90)\n",
    "for r in scan_results:\n",
    "    print(f\"{r[0]:<16} {r[1]:.4f} ± {r[2]:.4f}      {r[3]:.4f} ± {r[4]:.4f}\")\n",
    "print(\"=\"*90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee33830e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameter Tuning (using training data only)\n",
      "================================================================================\n",
      "\n",
      "Training LightGBM...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64779\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 273\n",
      "[LightGBM] [Info] Start training from score -0.124016\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM completed: CV RMSE=0.1008\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS (CV on Training Data):\n",
      "================================================================================\n",
      "LightGBM         RMSE=0.1008  params={'model__learning_rate': 0.01, 'model__max_depth': 10, 'model__n_estimators': 2000}\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "best_models = {}  # Store trained models\n",
    "\n",
    "print(\"\\nHyperparameter Tuning (using training data only)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "\n",
    "    # GridSearchCV on training data only (no data leakage)\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grids[name],\n",
    "        scoring=rmse_scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        gs.fit(X_train, y_train)\n",
    "        \n",
    "        all_results.append((name, -gs.best_score_, gs.best_params_))\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        print(f\"{name} completed: CV RMSE={-gs.best_score_:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR with {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "all_results = sorted(all_results, key=lambda x: x[1])  # lower RMSE better\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS (CV on Training Data):\")\n",
    "print(\"=\"*80)\n",
    "for name, score, params in all_results:\n",
    "    print(f\"{name:<16} RMSE={score:.4f}  params={params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8a2e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION SET EVALUATION - Checking for Overfitting\n",
      "================================================================================\n",
      "Model            Train RMSE   Val RMSE     Train R²   Val R²     RMSE Gap  \n",
      "================================================================================\n",
      "LightGBM         0.0259       0.0877       0.9871     0.8535     0.0617     ⚠️\n",
      "================================================================================\n",
      "Note: Large RMSE Gap or R² Gap indicates overfitting\n",
      "\n",
      "Best model by validation RMSE: LightGBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SET EVALUATION - Checking for Overfitting\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for name, model_estimator in best_models.items():\n",
    "    # Train predictions (already fitted on X_train)\n",
    "    y_train_pred = model_estimator.predict(X_train)\n",
    "    train_rmse = rmse(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Validation predictions (unseen data - detects overfitting)\n",
    "    y_val_pred = model_estimator.predict(X_val)\n",
    "    val_rmse = rmse(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Calculate overfitting gap\n",
    "    rmse_gap = val_rmse - train_rmse\n",
    "    r2_gap = train_r2 - val_r2\n",
    "    \n",
    "    validation_results.append((name, train_rmse, val_rmse, train_r2, val_r2, rmse_gap, r2_gap))\n",
    "\n",
    "# Sort by validation RMSE (best generalization)\n",
    "validation_results = sorted(validation_results, key=lambda x: x[2])\n",
    "\n",
    "print(f\"{'Model':<16} {'Train RMSE':<12} {'Val RMSE':<12} {'Train R²':<10} {'Val R²':<10} {'RMSE Gap':<10}\")\n",
    "print(\"=\"*80)\n",
    "for vr in validation_results:\n",
    "    gap_indicator = \"⚠️\" if vr[5] > 0.05 else \"\"  # Flag if validation is much worse\n",
    "    print(f\"{vr[0]:<16} {vr[1]:<12.4f} {vr[2]:<12.4f} {vr[3]:<10.4f} {vr[4]:<10.4f} {vr[5]:<10.4f} {gap_indicator}\")\n",
    "print(\"=\"*80)\n",
    "print(\"Note: Large RMSE Gap or R² Gap indicates overfitting\")\n",
    "\n",
    "best_name_val = validation_results[0][0]\n",
    "print(f\"\\nBest model by validation RMSE: {best_name_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96dc534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL MODEL TRAINING ON FULL DATA\n",
      "================================================================================\n",
      "Retraining LightGBM on full training data...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 64779\n",
      "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 273\n",
      "[LightGBM] [Info] Start training from score -0.123096\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model: LightGBM\n",
      "Full data RMSE: 0.0300\n",
      "Full data R²: 0.9828\n",
      "Best params: {'model__learning_rate': 0.01, 'model__max_depth': 10, 'model__n_estimators': 2000}\n",
      "\n",
      "Saved: EVAL_target01_36.csv\n",
      "Predictions shape: (10000,)\n",
      "Predictions range: [-0.4805, 0.3249]\n"
     ]
    }
   ],
   "source": [
    "# Retrain best model on FULL training data for final predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL TRAINING ON FULL DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use best model from validation results\n",
    "best_model_class = models[best_name_val]\n",
    "best_params_dict = next(params for name, _, params in all_results if name == best_name_val)\n",
    "\n",
    "# Create fresh pipeline with best hyperparameters\n",
    "final_pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", best_model_class)])\n",
    "final_pipe.set_params(**best_params_dict)\n",
    "\n",
    "# Train on ALL training data (X, y1) for maximum performance\n",
    "print(f\"Retraining {best_name_val} on full training data...\")\n",
    "final_pipe.fit(X, y1)\n",
    "\n",
    "# Calculate final metrics on full data\n",
    "y_full_pred = final_pipe.predict(X)\n",
    "full_rmse = rmse(y1, y_full_pred)\n",
    "full_r2 = r2_score(y1, y_full_pred)\n",
    "\n",
    "print(f\"\\nFinal model: {best_name_val}\")\n",
    "print(f\"Full data RMSE: {full_rmse:.4f}\")\n",
    "print(f\"Full data R²: {full_r2:.4f}\")\n",
    "print(f\"Best params: {best_params_dict}\")\n",
    "\n",
    "# Generate predictions for EVAL set\n",
    "eval_pred = final_pipe.predict(X_eval)\n",
    "\n",
    "# Save predictions\n",
    "output_filename = f\"EVAL_target01_{PROBLEM_NUM}.csv\"\n",
    "submission = pd.DataFrame({\"target01\": eval_pred})\n",
    "submission.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nSaved: {output_filename}\")\n",
    "print(f\"Predictions shape: {eval_pred.shape}\")\n",
    "print(f\"Predictions range: [{eval_pred.min():.4f}, {eval_pred.max():.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5445f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45cb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
