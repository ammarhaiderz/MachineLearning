{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbaaa7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 36\n",
      "X: (10000, 273), y1: (10000,), X_eval: (10000, 273)\n",
      "\n",
      "Train/Val Split:\n",
      "X_train: (8000, 273), y_train: (8000,)\n",
      "X_val: (2000, 273), y_val: (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Configure problem number here\n",
    "PROBLEM_NUM = 36\n",
    "\n",
    "X_path = f\"./data_31_40/problem_{PROBLEM_NUM}/dataset_{PROBLEM_NUM}.csv\"\n",
    "y_path = f\"./data_31_40/problem_{PROBLEM_NUM}/target_{PROBLEM_NUM}.csv\"\n",
    "Xeval_path = f\"./data_31_40/problem_{PROBLEM_NUM}/EVAL_{PROBLEM_NUM}.csv\"\n",
    "\n",
    "X = pd.read_csv(X_path)\n",
    "y = pd.read_csv(y_path)\n",
    "X_eval = pd.read_csv(Xeval_path)\n",
    "\n",
    "y1 = y[\"target01\"]\n",
    "\n",
    "print(f\"Problem {PROBLEM_NUM}\")\n",
    "print(f\"X: {X.shape}, y1: {y1.shape}, X_eval: {X_eval.shape}\")\n",
    "assert list(X.columns) == list(X_eval.columns), \"Train/EVAL column mismatch!\"\n",
    "\n",
    "# Create train/validation split to detect overfitting\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y1, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Val Split:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8537e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Selection to Remove Noise ===\n",
      "\n",
      "Low-variance filter: 273 → 273 features\n",
      "Removed 0 low-variance features\n",
      "\n",
      "F-test filter (p<0.05): 273 → 35 features\n",
      "Removed 238 statistically insignificant features\n",
      "\n",
      "Mutual Info filter (>25th percentile): kept 141 features\n",
      "Removed 132 low-information features\n",
      "\n",
      "=== FINAL (F-test AND MI): 273 → 24 features ===\n",
      "Removed 249 likely noise features\n",
      "\n",
      "Filtered shapes:\n",
      "X_train: (8000, 24)\n",
      "X_val: (2000, 24)\n",
      "X_eval: (10000, 273) (unchanged)\n"
     ]
    }
   ],
   "source": [
    "# Feature selection: Remove noise features\n",
    "from sklearn.feature_selection import VarianceThreshold, f_regression, mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== Feature Selection to Remove Noise ===\\n\")\n",
    "\n",
    "# Method 1: Remove low-variance features (near-constant)\n",
    "selector = VarianceThreshold(threshold=0.005)  # Remove features with variance < 0.005\n",
    "X_train_var = selector.fit_transform(X_train)\n",
    "low_var_mask = selector.get_support()\n",
    "print(f\"Low-variance filter: {X_train.shape[1]} → {X_train_var.shape[1]} features\")\n",
    "print(f\"Removed {(~low_var_mask).sum()} low-variance features\\n\")\n",
    "\n",
    "# Method 2: Statistical test - F-test (linear relationship with target)\n",
    "# Higher F-score = more likely to be signal, not noise\n",
    "f_scores, p_values = f_regression(X_train, y_train)\n",
    "# Keep features with p-value < 0.05 (statistically significant)\n",
    "f_test_mask = p_values < 0.05\n",
    "print(f\"F-test filter (p<0.05): {X_train.shape[1]} → {f_test_mask.sum()} features\")\n",
    "print(f\"Removed {(~f_test_mask).sum()} statistically insignificant features\\n\")\n",
    "\n",
    "# Method 3: Mutual Information (captures non-linear relationships too)\n",
    "mi_scores = mutual_info_regression(X_train, y_train, random_state=42)\n",
    "# Keep features with MI > 0 (some information about target)\n",
    "mi_threshold = np.percentile(mi_scores, 25)  # Remove bottom 25%\n",
    "mi_mask = mi_scores > mi_threshold\n",
    "print(f\"Mutual Info filter (>25th percentile): kept {mi_mask.sum()} features\")\n",
    "print(f\"Removed {(~mi_mask).sum()} low-information features\\n\")\n",
    "\n",
    "# FIXED: Use AND logic - keep features that pass BOTH F-test AND MI test\n",
    "combined_mask = f_test_mask & mi_mask\n",
    "good_features = X_train.columns[combined_mask]\n",
    "\n",
    "print(f\"=== FINAL (F-test AND MI): {X_train.shape[1]} → {len(good_features)} features ===\")\n",
    "print(f\"Removed {X_train.shape[1] - len(good_features)} likely noise features\\n\")\n",
    "\n",
    "# Apply filtering - NOTE: X_eval is NOT filtered (keep original)\n",
    "X_train_filtered = X_train[good_features]\n",
    "X_val_filtered = X_val[good_features]\n",
    "\n",
    "print(f\"Filtered shapes:\")\n",
    "print(f\"X_train: {X_train_filtered.shape}\")\n",
    "print(f\"X_val: {X_val_filtered.shape}\")\n",
    "print(f\"X_eval: {X_eval.shape} (unchanged)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ad17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5185ffac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- feat_0\n- feat_1\n- feat_10\n- feat_100\n- feat_101\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m X_val_scaled = scaler.transform(X_val_filtered)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# X_eval uses original unfiltered data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m X_eval_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Step 2: Apply PCA on scaled data\u001b[39;00m\n\u001b[32m     13\u001b[39m pca = PCA(n_components=\u001b[32m0.95\u001b[39m)  \u001b[38;5;66;03m# retain 95% variance\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1094\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1091\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1093\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2877\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2793\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2794\u001b[39m     _estimator,\n\u001b[32m   2795\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2801\u001b[39m     **check_params,\n\u001b[32m   2802\u001b[39m ):\n\u001b[32m   2803\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2804\u001b[39m \n\u001b[32m   2805\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2875\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2876\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2877\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2878\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2879\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repositories\\Github-MachineLearning\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2729\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2726\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2727\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2729\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- feat_0\n- feat_1\n- feat_10\n- feat_100\n- feat_101\n- ...\n"
     ]
    }
   ],
   "source": [
    "# feature engineering and reduction\n",
    "# Step 1: Scale features first (PCA is sensitive to scale!)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)  # Use filtered features\n",
    "X_val_scaled = scaler.transform(X_val_filtered)\n",
    "# X_eval uses original unfiltered data\n",
    "X_eval_scaled = scaler.transform(X_eval)\n",
    "\n",
    "# Step 2: Apply PCA on scaled data\n",
    "pca = PCA(n_components=0.95)  # retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_eval_pca = pca.transform(X_eval_scaled)\n",
    "\n",
    "print(f\"\\nAfter Scaling + PCA Transformation:\")\n",
    "print(f\"Filtered features: {X_train_filtered.shape[1]}\")\n",
    "print(f\"PCA components: {X_train_pca.shape[1]} (retaining {pca.explained_variance_ratio_.sum():.1%} variance)\")\n",
    "print(f\"X_train_pca: {X_train_pca.shape}\")\n",
    "print(f\"X_val_pca: {X_val_pca.shape}\")\n",
    "print(f\"X_eval_pca: {X_eval_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e4fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
